{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQsZN80nsRrsylpJB++y4R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**1. Download single cell metadata**\n","\n"," Since single cell metadatas take on different data types including .mtx, .csv and .tsv, an integrated downloading process is efficient for further research. Below are brief processes to download the desired data, respectively.\n","\n"," In Case of .mtx (mostly from 2020 or later), follow A.\n"," For .tsv, follow B-1 through 3. For .csv, skip B-2. Concatenation takes considerable data storage. When it comes short, don't follow B-4. Otherwise follow B-4 to merge data into one.\n","\n"," WARNING : Always set raw_dir in /contents/, not in your drive. It may cause undesirable storage overflow."],"metadata":{"id":"5F8jyRwn7ZpG"}},{"cell_type":"code","source":["import torch\n","\n","# Single Cell Libraries\n","import scvi\n","import scanpy as sc\n","import anndata as ad\n","\n","# Data Processing and Plotting\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import igraph\n","import leidenalg\n","\n","# File grab\n","import os\n","import tempfile\n","import pooch\n","import shutil, subprocess, glob\n","import gzip\n","\n","print(torch.__version__)\n","print(scvi.__version__)\n","print(torch.cuda.is_available())"],"metadata":{"id":"NAYlADIBtNYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set data information\n","FNAME = \"GSE140393_raw\"\n","DATA_INFO = {\n","    \"FNAME\": FNAME,\n","    \"LINK\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE140393&format=file\",\n","    \"LABEL\": \"tsankova\",\n","    \"RAW_DIR\": f\"/content/data/{FNAME}\",\n","    \"OUT_DIR\": f\"/content/drive/MyDrive/datas/epilepsy_microglia/raw/{FNAME}\"\n","}\n","os.makedirs(DATA_INFO[\"RAW_DIR\"], exist_ok=True)\n","os.makedirs(DATA_INFO[\"OUT_DIR\"], exist_ok=True)"],"metadata":{"id":"AmAkVipJ1TBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A. downloading mtx type data\n","\n","LINK = DATA_INFO[\"LINK\"]\n","FNAME = DATA_INFO[\"FNAME\"]\n","RAW_DIR = DATA_INFO[\"RAW_DIR\"]\n","OUT_DIR = DATA_INFO[\"OUT_DIR\"]\n","LABEL = DATA_INFO[\"LABEL\"]\n","\n","# download + unpack\n","subprocess.run([\"curl\", \"-L\", LINK, \"-o\", f\"{FNAME}.tar\"], check=True)\n","subprocess.run([\"tar\", \"-xf\", f\"{FNAME}.tar\", \"-C\", RAW_DIR], check=True)\n","os.remove(f\"{FNAME}.tar\")\n","# normalize + regroup\n","files = glob.glob(os.path.join(RAW_DIR, \"*\"))\n","groups = {}\n","for f in files:\n","    fname_only = os.path.basename(f)\n","    prefix = fname_only.split(\"_\")[0]\n","    groups.setdefault(prefix, []).append(f)\n","for prefix, flist in groups.items():\n","    sample_dir = os.path.join(OUT_DIR, prefix)\n","    os.makedirs(sample_dir, exist_ok=True)\n","    for f in flist:\n","        fname_only = os.path.basename(f)\n","        if \"matrix.mtx\" in fname_only:\n","            # check gunzip file\n","            if fname_only.endswith(\".gz\"):\n","                dest = os.path.join(sample_dir, \"matrix.mtx.gz\")\n","            else:\n","                dest = os.path.join(sample_dir, \"matrix.mtx\")\n","        elif \"barcodes.tsv\" in fname_only:\n","            if fname_only.endswith(\".gz\"):\n","                dest = os.path.join(sample_dir, \"barcodes.tsv.gz\")\n","            else:\n","                dest = os.path.join(sample_dir, \"barcodes.tsv\")\n","        elif \"genes.tsv\" in fname_only or \"features.tsv\" in fname_only:\n","            if fname_only.endswith(\".gz\"):\n","                dest = os.path.join(sample_dir, \"features.tsv.gz\")\n","            else:\n","                dest = os.path.join(sample_dir, \"features.tsv\")\n","        else:\n","            dest = os.path.join(sample_dir, fname_only)\n","        shutil.move(f, dest)\n","\n","# build AnnData per dataset\n","adatas = []\n","for folder in sorted(glob.glob(os.path.join(OUT_DIR, \"*\"))):\n","    if os.path.isdir(folder):\n","        print(f\"Reading {folder}\")\n","        ad = sc.read_10x_mtx(folder, var_names=\"gene_symbols\")\n","        ad.obs[\"sample\"] = os.path.basename(folder)\n","        adatas.append(ad)\n","if adatas:\n","    adata = adatas[0].concatenate(adatas[1:], join=\"outer\", batch_key=\"sample_id\")\n","    print(adata)\n","    adata.write(f\"{OUT_DIR}/{LABEL}_{FNAME}.h5ad\")\n","    for f in glob.glob(os.path.join(OUT_DIR, \"*\")):\n","        if os.path.isdir(f):\n","            shutil.rmtree(f)\n","else:\n","    print(\"No samples found!\")"],"metadata":{"id":"goGqpAg4t2zo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# B-1. downloading csv/tsv type data\n","\n","LINK = DATA_INFO[\"LINK\"]\n","FNAME = DATA_INFO[\"FNAME\"]\n","RAW_DIR = DATA_INFO[\"RAW_DIR\"]\n","OUT_DIR = DATA_INFO[\"OUT_DIR\"]\n","LABEL = DATA_INFO[\"LABEL\"]\n","\n","subprocess.run([\"curl\", \"-L\", LINK, \"-o\", f\"{FNAME}.tar\"], check=True)\n","subprocess.run([\"tar\", \"-xf\", f\"{FNAME}.tar\", \"-C\", RAW_DIR], check=True)\n","os.remove(f\"{FNAME}.tar\")\n","\n","for f in glob.glob(os.path.join(RAW_DIR, \"*.gz\")):\n","    fname_only = os.path.basename(f)\n","    outpath = os.path.join(RAW_DIR, fname_only[:-3])  # remove \".gz\"\n","    with gzip.open(f, 'rb') as f_in, open(outpath, 'wb') as f_out:\n","        shutil.copyfileobj(f_in, f_out)\n","    os.remove(f)"],"metadata":{"id":"gb-J1fyNus0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# B-2. transforming tsv to csv\n","\n","for tsvpath in glob.glob(os.path.join(RAW_DIR, \"*.tsv\")):\n","    fname_only = os.path.splitext(os.path.basename(tsvpath))[0]\n","    csvpath = os.path.join(OUT_DIR, f\"{fname_only}.csv\")\n","    try:\n","        df = pd.read_csv(tsvpath, sep=\"\\t\")\n","        df.to_csv(csvpath, index=False)\n","        os.remove(tsvpath)  # delete original.tsv\n","        print(f\"Converted {tsvpath} -> {csvpath} (and deleted original)\")\n","    except Exception as e:\n","        print(f\"Error processing {tsvpath}: {e}\")"],"metadata":{"id":"7jPaBqi8u9YZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# B-3. Transforming csv into h5ad\n","\n","# Check cell-gene orientation\n","def check_csv_orientation(path, n_check=5):\n","    df = pd.read_csv(path, index_col=0, nrows=n_check)\n","    n_rows, n_cols = df.shape\n","\n","    row_idx = df.index[0]\n","    col_idx = df.columns[0]\n","\n","    if row_idx.startswith(\"ENSG\") or row_idx.isalpha():\n","        orientation = \"genes_as_rows\"\n","    elif col_idx.startswith(\"ENSG\") or col_idx.isalpha():\n","        orientation = \"genes_as_columns\"\n","    else:\n","        orientation = \"unknown\"\n","\n","    print(f\"{path}: {orientation} ({n_rows}X{n_cols})\")\n","    return orientation\n","\n","intermediate_files = []\n","\n","# Read .csv into .h5ad\n","for f in glob.glob(os.path.join(RAW_DIR, \"*.csv\")):\n","  fname_only = os.path.splitext(os.path.basename(f))[0]\n","\n","  ori = check_csv_orientation(f)\n","\n","  if ori == \"genes_as_rows\":\n","    adata = sc.read_csv(f, first_column_names=True)\n","    adata = adata.T\n","  elif ori == \"genes_as_columns\":\n","    adata = sc.read_csv(f, first_column_names=True)\n","  else:\n","    raise ValueError(f\"Unknown orientation for {f}\")\n","\n","  adata.obs[\"sample\"] = fname_only\n","  tmp_path = os.path.join(OUT_DIR, f\"{fname_only}.h5ad\")\n","  adata.write(tmp_path)\n","  intermediate_files.append(tmp_path)\n","\n","  print(f\"Converted {f} -> {tmp_path}\")\n","  del adata\n","  os.remove(f)"],"metadata":{"id":"uWYJW2VSvEbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# B-4. Merge adatas\n","\n","adatas = [sc.read_h5ad(f) for f in intermediate_files]\n","\n","adata_merged = sc.concat(adatas, join=\"outer\", label=\"sample\", keys=[os.path.basename(f).replace(\".h5ad\",\"\") for f in intermediate_files])\n","\n","merged_path = os.path.join(OUT_DIR, f\"{LABEL}_{FNAME}_merged.h5ad\")\n","adata_merged.write(merged_path)"],"metadata":{"id":"yDRHzy_pvKTF"},"execution_count":null,"outputs":[]}]}